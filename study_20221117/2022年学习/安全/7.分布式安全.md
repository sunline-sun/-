##### 以Hadoop为例

Hadoop默认在可信环境下，所以本身安全性不高。

##### 黑客能做什么

- 因为Hadoop没有集成认证和授权，所以可以直接通过客户端连接服务端，然后任意增删改查，任意提交任务
- 因为Hadoop集群中传输没有加密，所以黑客可以通过控制交换机等设备，获取数据
- 黑客可以以一个节点的身份加入到Hadoop集群中，可以对整个集群的资源进行调度

##### Hadoop的安全功能

- 认证：Hadoop 支持了基于 Kerberos 协议的认证功能

  - <img src="https://static001.geekbang.org/resource/image/e7/5e/e705c8543a017c24ed107fac3e8f4d5e.jpeg?wh=1920*1080" alt="img" style="zoom: 25%;" />

  - Kerberos 比较适用于服务与服务之间的认证，对应到 Hadoop 的场景中，就是 Hadoop 集群中内部各个节点之间的认证。
  - 我们在初始化 Hadoop 的各个节点时，为每个节点申请一个 Kerberos 的密钥文件 Keytab。
  - Keytab 文件会使用一个 Principal 作为唯一的身份标识。Principal 的格式如下：username/host@realm。
  - 通过 Principal，Keytab 文件会和节点的服务类型以及 Host 进行绑定。这样一来，每个服务节点都具备了能证实身份的唯一 ID 和密钥，也就可以保证在整个 Hadoop 集群中，各个节点都是可信任的。

- 授权

  - 当认证开启后，只要用户登录一台配置好了 Kerberos 密钥的服务器，就能以节点的身份向 Hadoop 发起认证了。

- 加密

  - 支持对硬盘数据进行加密存储，这个过程主要集中在 HDFS 中

  - 在 HDFS 中，则需要我们主动创建 Zone 来进行加密。

    - ```
      对zone下的文件加密
      hadoop fs -mkdir /zone
      hdfs crypto -createZone -keyName mykey -path /zone
      ```

- 使用安全框架加强安全功能

  - Apache Knox：针对Hadoop集群的网关，作为集群的统一入口，提供认证、授权、审计功能
    - <img src="https://static001.geekbang.org/resource/image/8d/1c/8d9af830e8d2fdf8966e16de091e3a1c.jpeg?wh=1920*1080" alt="img" style="zoom:25%;" />
  - Apache Sentry：授权中心，在Hive等引擎中添加插件，针对所有请求，转发到授权中心，优点是可拔插的，耦合性低
    - <img src="https://static001.geekbang.org/resource/image/38/93/387baa6f9f027379f15ccf5abc1d3793.jpeg?wh=1920*1080" alt="img" style="zoom:25%;" />

  - Apache Ranger：集中式的访问控制机制，可以通过他的管理后台，管理各类资源，他是通过一个轻量级的java插件，运行在服务进程中
    - <img src="https://static001.geekbang.org/resource/image/a1/c1/a12efec9e5ce98dd24e2e81d344fd7c1.jpeg?wh=1920*1080" alt="img" style="zoom:25%;" />

  - <img src="https://static001.geekbang.org/resource/image/37/b1/37d908d9f3b3cd32e51d816acbb602b1.jpeg?wh=1920*1080" alt="img" style="zoom: 33%;" />
  - 推荐你使用 Apache Ranger 和 Apache Knox 的组合。因为 Apache Ranger 和 Apache Knox 是同一个公司（Hortonworks）推出的安全框架，它们在功能上是相辅相成。