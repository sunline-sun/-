数据采集主要包括数据库数据、日志数据、埋点数据、爬虫数据



#### 数据库导出到Hadoop工具

##### Sqoop

Sqoop适合关系数据库数据的批量导入

```
$ sqoop import --connect jdbc:mysql://localhost/db --username foo --password --table TEST
```



##### Canal

Canal是阿里巴巴开源的一个MySQL binLog获取工具，适合实时导入数据库数据到Hadoop

他的工作原理就是将自己伪装为MySQL从库，从MySQL获取binlog

![img](https://static001.geekbang.org/resource/image/f8/6d/f84e49e679c9444812200ba0b079ce6d.png?wh=1066*380)



#### 日志文件导入

##### Flume

Flume是大数据日志收集常用的工具，由Cloudera开发，后来捐赠给Apache基金会

##### Flume架构

![img](https://static001.geekbang.org/resource/image/33/76/33e564d2c4f292584eab32c488f13a76.png?wh=958*426)

- Flume收集日志的核心组件是Flume Agent，负责将日志从数据源收集起来并保存到大数据存储设备。
- Agent Source 负责收集日志数据，支持从Kafka、本地日志文件、Socket通讯端口、Unix标准输出、Thrift等各种数据源获取日志数据。
- Source收到数据后，将数据封装为event事件，发送给Channel
- Channel是一个队列，有内存、磁盘、数据库等几种实现方式。主要用来对event事件消息排队，然后发送给Sink
- Sink收到数据后，将数据输出保存到大数据存储设备，比如HDFS、HBase等，
- Sink的输出可以作为Source的输入，这样Agent就可以级联起来，组成各种结构了

![img](https://static001.geekbang.org/resource/image/05/bf/057e3a89a22cc6a77c9c892b7cdd4ebf.png?wh=1254*848)





#### 前端埋点采集

用户某些行为不会产生后端请求，比如用户在一个页面停留的时间、用户拖动页面速度、用户选择一个复选框又取消了，这些对于大数据处理，用于分析用户行为，进行智能推荐都有价值，所以需要前端埋点获取这些数据。



埋点分为手工埋点、自动化埋点、可视化埋点

##### 手工埋点

手工埋点就是前端开发者手动编程将需要采集的数据发送给后端数据采集系统。直接通过调用接口的方式将ID、名称、控件、页面等参数发送给后端存储。



##### 自动化埋点

指的自动收集全部用户操作事件，也叫无埋点，也就是无需埋点，也是全埋点，对用户所有操作进行采集，发送给后端服务器。

这种方式的好处是开发工作量小，缺点是采集数据量太大，很多无用数据，浪费计算资源，而且消耗用户流量。



##### 可视化埋点

其实就是通过配置的方式，配置前端哪些操作需要埋点，根据配置采集数据。



#### 爬虫系统

通过网络爬虫获取外部数据也是公司大数据的重要来源之一。

爬虫的过程中也会遇到反爬虫策略，这时候要看爬虫代价和爬虫价值的比较了，是否价值大于代价。

- 反爬虫策略：
  -  网页时代，验证header&签名，动态加载，反selenium/phantomjs，ip封禁，有毒数据，动态爬虫阈值（过了阈值后依然允许爬一阵再封禁），各种验证码，云厂商反爬模式识别 
  - app时代，ios和安卓的反逆向，比如安卓的加壳，代码混淆，强制登录token，账户管理，反抓包（ssl pin），包签名校验，反注入（监测），so，LLVM混淆，反Hook，异常账号识别，模式识别 
- 应对这些反爬虫策略: 
  - 网页，从简单的header伪装，机器学习验证码，验证码打码平台，ip代理商，反动态抓取校验，阈值报警，多策略爬取校验
  - 应对手机反爬：这个是逆向安全团队，加壳有脱壳，账号有养账号，短信打码平台，反抓包有xposed切面hook，反sslpinning，签名校验有调试关闭，so包有模拟环境调用，IDA调试。脚本精灵抓包。