分类是大数据常见的应用场景，通过对历史数据规律的统计，将大量数据进行分类然后发现数据之间的关系，当有新的数据进来时候，计算机可以利用这个关系自动进行分类。



### KNN分类算法

KNN算法，即K近邻（K Nearest Neighbour）算法，是一种基本的分类算法

##### 原理

对于一个需要分类的数据，将其和一组已经分类标注好的样本集合进行比较，得到距离最近的K个样本，K个样本最多归属的类别，就是这个需要分类数据的类别。

![img](https://static001.geekbang.org/resource/image/ef/d4/ef51184bbc5ecf8e0eb71c8c834557d4.png?wh=1156*1054)



- 红蓝绿三种颜色的点为样本数据，分属三个类别w1、w2、w3，对于待分类点Xu，计算和他距离最近的五个点，这五个点最多归属的类别为w1，所以Xu的类别被分类为w1



##### 应用场景

- 新闻分类，人工标注后，计算好特征向量
- 商品分类
- 简单的文字识别





#### 数据的距离

KNN算法的关键就是怎么计算数据的距离，通常做法是提取数据的特征值，根据特征值组成一个n维实数向量空间（特征空间），然后计算向量之间的空间距离。常用的计算方法有欧式距离、余弦距离等



#### 欧式距离计算

对于数据 xi 和 xj，若其特征空间为 n 维实数向量空间 Rn，即 xi=(xi1,xi2,…,xin)，xj=(xj1,xj2,…,xjn)，则其欧氏距离计算公式为

![image-20220801183347301](C:\Users\sunyong\AppData\Roaming\Typora\typora-user-images\image-20220801183347301.png)

平面几何和立体几何两点距离也是这个公式，只是平面几何n=2，立体几何n=3，而机器学习每个数据都有n维的维度



#### 余弦距离计算

![image-20220801184012288](C:\Users\sunyong\AppData\Roaming\Typora\typora-user-images\image-20220801184012288.png)

余弦相似度值越接近1表示其越相似，越接近0代表其差异越大，使用余弦相似度可以消除数据的某些冗余信息

举例：比如两篇文章的特征值都是“大数据”，“机器学习”，“极客时间”，A文章的特征向量是（3,3,3），即这三个词出现的频率是3，B文章的特征向量是（6,6,6），如果光看特征向量，这两个向量差别很大，如果用欧式距离计算确实也很大，但是这两篇文章其实非常类似，只是篇幅不同，他们的余弦相似度为1，表示非常相似。





#### 文本的特征值

特征值就是提取数据的特征值，用来知道数据的特征向量，从而计算距离。

文本数据的特征值就是提取文本关键词，TF-IDF算法是比较常用的的一种文本关键词提取算法，由TF和IDF两部分构成。

- TF 是词频，表示某个单词在文档中出现的频率，一个单词在一个文档出现越
  - TF=某个词在文档中出现的次数/文档总词数
- IDF是逆文档频率，表示这个单词在所有文档中的稀缺程度，越少文档出现这个词，IDF值越高。
  - IDF=log(出现该词的文档数所有的文档总数)



